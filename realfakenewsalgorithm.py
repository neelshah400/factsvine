# -*- coding: utf-8 -*-
"""Realfakenewsalgorithm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VjtzoZp27lxZIAGQyCRtcWyL_rKCbkUg

# Solving the Fake News Challenge

Welcome to the third and final week of AI4ALL. Let's get started.

Below you will find the code you (hopefully) wrote last week.
"""

# Commented out IPython magic to ensure Python compatibility.
# -*- coding: utf-8 -*-
"""
Created on Tue Jul  2 16:50:39 2019
@author: Nobline
"""
#####################THIS IS THE LOCAL ONE ON MY COMPUTER
import pandas as pd
import numpy as np
import os


bodies = pd.read_csv("train_bodies.csv")
headlines = pd.read_csv("train_stances.csv")

import re
from gensim.parsing.preprocessing import remove_stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer

def clean(doc):
    for i in range(doc.shape[0]):
        #lowercasing
        doc.set_value(i, doc.iloc[i].lower())
    #print("LOWERCASE")
    #print(doc)
    for i in range(doc.shape[0]):
        #remove punctuation
        doc.set_value(i, re.sub(r'([^\s\w])+', '', doc.iloc[i]))
    #print("REMOVE PUNCT")
    #print(doc)
    for i in range(doc.shape[0]):
        #remove stopwords
        doc.set_value(i, remove_stopwords(doc.iloc[i]))
    #print("REMOVE STOPWORDS")
    #print(doc)
    for i in range(doc.shape[0]):
        #tokenize
        doc.set_value(i, word_tokenize(doc.iloc[i]))
    #print("TOKENIZE")
    #print(doc)
    for i in range(doc.shape[0]):
        #lemmatize
        for j in range(len(doc.iloc[i])):
            doc.iloc[i][j] = lemmatizer.lemmatize(doc.iloc[i][j])
    #print("LEMMATIZE")
    #print(doc)
    
#extract tfidf vectors of article body and headline
def default(doc):
    return doc

def tfidf_extractor(b, h):
    global bodyTFIDF
    global headlineTFIDF
    vect = TfidfVectorizer(max_features = 500, analyzer = 'word', preprocessor = default, tokenizer=default, token_pattern = None)
    
    vocab = h + b
    vocabTFIDF = vect.fit(vocab)
    
    bodyTFIDF = vect.transform(b)
    #print results
    #print("BODY VOCABULARY")
    #print(vect.vocabulary_)
    #print("BODIES")
    #print(bodyTFIDF)
    
    headlineTFIDF = vect.transform(h)
    #print results
    #print("HEADLINE VOCABULARY")
    #print(vect.vocabulary_)
    #print("HEADLINES")
    #print(headlineTFIDF)
def count_vectorizer(b, h):
    global vect
    vect = CountVectorizer(max_features = 1000, lowercase = False, analyzer = 'word', preprocessor = default, tokenizer=default, token_pattern = None)
    vect.fit(b + h)
def count_vectorizer2(b, h):
    global vect2
    vect2 = CountVectorizer(max_features = 1000, lowercase = False, analyzer = 'word', preprocessor = default, tokenizer=default, token_pattern = None)
    vect2.fit(b + h)
#Unrelated Related sorter
def unrelatedRelated(data):
    data['new_stance'] = data['Stance']
    data.loc[data['Stance'].isin(['agree', 'disagree', 'discuss']), 'new_stance'] = 'related'

def test_predictionsUnrelated(data):
    global stance_correct_counts
    global stance_counts
    for i in range(data.shape[0]):
        predicted_stance = data.iloc[i]['predicted_stance']
        actual_stance = data.iloc[i]['new_stance']
        stance_counts[predicted_stance] += 1
        stance_counts['FNC'] += .25
        if predicted_stance == actual_stance:
            stance_correct_counts[actual_stance] += 1
            stance_correct_counts['FNC'] += .25
    for stance in stance_counts.keys():
        if stance_counts[stance] == 0:
            stance_counts[stance] = 1
    return {"unrelated":stance_correct_counts["unrelated"]/stance_counts["unrelated"], "related":stance_correct_counts["related"]/stance_counts["related"], "FNC":stance_correct_counts['FNC']/stance_counts['FNC']}

def test_predictionsRelated(data):
    global stance_correct_counts
    global stance_counts
    for i in range(data.shape[0]):
        predicted_stance = data.iloc[i]['predicted_stance']
        actual_stance = data.iloc[i]['Stance']
        stance_counts[predicted_stance] += 1
        stance_counts['FNC'] += .75
        if predicted_stance == actual_stance:
            stance_correct_counts[actual_stance] += 1
            stance_correct_counts['FNC'] += .75
    for stance in stance_counts.keys():
        if stance_counts[stance] == 0:
            stance_counts[stance] = 1
    return {"discuss":stance_correct_counts["discuss"]/stance_counts["discuss"], "agree":stance_correct_counts["agree"]/stance_counts["agree"], "disagree":stance_correct_counts["disagree"]/stance_counts["disagree"], "FNC":stance_correct_counts['FNC']/stance_counts['FNC']}

#Clean Docs
import nltk
nltk.download('wordnet')
nltk.download('punkt')
clean(bodies['articleBody'])
clean(headlines['Headline'])
train_data = pd.merge(bodies, headlines, on='Body ID')
train_data

"""Run down of what we'll be doing during the rest of the week.

1. **Implement** cosine similarity.
2. **Train** our Support Vector Machine.
3. **Make** predictions.
4. **Evaluate** our model.
4. **Present** our findings!

Let's get started! Open up the Resources document, and peruse the links. This document is for you, so please, please add resources as you see fit. And please work with the people around you. This is a **collaborative**, *research* project.

Here we provide the functional framework for the project. Your job is to write code to complete the functions you need. It is up to you to use the framework and to pick and choose what features you would like to use (feature engineering). Feel free to explore LDA, TFIDF, cosine similarity, etc. Explore!
"""

from sklearn.metrics.pairwise import paired_cosine_distances as pcd

#calculate the similarity between each pair of document and headline
def cosDistance(b,h):
    #Cosine Similarity
    global cosSimilarity
    cosSimilarity = pcd(bodyTFIDF, headlineTFIDF)
    return(cosSimilarity)

from sklearn.decomposition import LatentDirichletAllocation
from sklearn.model_selection import GridSearchCV

def LDA(b,h):
    global bodyLDA
    global headlineLDA
    global lda
    lda = LatentDirichletAllocation(n_components=10, n_jobs= -1, max_iter=5, random_state=0, learning_offset=50)
    bodyLDA = lda.fit_transform(b)
    headlineLDA = lda.fit_transform(h)
    return (bodyLDA, headlineLDA)
def gs_lda(b,h):
    parameters = {'n_components': [10, 15, 20, 25, 30], 'learning_decay': [.5, .7, .9]}
    gs_clf = GridSearchCV(lda, parameters, n_jobs = -1)
    gs_clf.fit(LDA(b, h))
    GridSearchCV(cv=None, error_score='raise', estimator=LatentDirichletAllocation(batch_size=128, doc_topic_prior=None, evaluate_every=-1, learning_decay=0.7, learning_method=None, learning_offset=10.0, max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001, n_components=10, n_jobs=1, n_topics=None, perp_tol=0.1, random_state=None, topic_word_prior=None, total_samples=1000000.0, verbose=0), fit_params=None, iid=True, n_jobs=1, param_grid={'n_topics': [10, 15, 20, 25, 30], 'learning_decay': [0.5, 0.7, 0.9]}, pre_dispatch='2*n_jobs', refit=True, return_train_score='warn', scoring=None, verbose=0)
    print (model.best_estimator_)
    print (gs_clf.best_score_)
    print (gs_clf.best_params_)

#train SVM classifier
from sklearn.linear_model import SGDClassifier
from scipy import sparse
def svmClass(b_tfidf, h_tfidf, b_lda, h_lda, b_tf, h_tf, labels):
    global clf
    clf = SGDClassifier(loss = 'hinge', penalty = 'l2', n_jobs = -1, max_iter=10000, tol=0.001) #, 'alpha'= 0.001, 'eta0'= 0.2,'learning_rate'= 'invscaling', 'power_t'= 0.1
    cos_sim = cosDistance(b_tfidf, h_tfidf).reshape(-1,1)
    cos_sim2 = cosDistance(b_lda, h_lda).reshape(-1,1)
    b_tf2, h_tf2 = b_tf.toarray(), h_tf.toarray()
    clf.fit(np.concatenate([cos_sim, cos_sim2, b_tf2, h_tf2], axis=1), labels)
def svmClass2(b_tfidf, h_tfidf, b_lda, h_lda, b_tf, h_tf, labels):
    global clf2
    clf2 = SGDClassifier(loss = 'hinge', penalty = 'l2', n_jobs = -1, max_iter=1000, tol=0.001, alpha= 0.001, eta0= 0.07, learning_rate= 'invscaling', power_t= 0.1)
    cos_sim = cosDistance(b_tfidf, h_tfidf).reshape(-1,1)
    cos_sim2 = cosDistance(b_lda, h_lda).reshape(-1,1)
    b_tf2, h_tf2 = b_tf.toarray(), h_tf.toarray()
    clf2.fit(np.concatenate([cos_sim, cos_sim2, b_tf2, h_tf2], axis=1), labels)
def gs_svmClass(b_tfidf, h_tfidf, b_lda, h_lda, b_tf, h_tf, labels):
    parameters = {'loss': ('hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'), 'penalty': ('l2', 'l1', 'elasticnet')}#, 'alpha' : (1e-2, 1e-3, 5e-2, 5e-3), 'learning_rate' : ('constant', 'optimal', 'invscaling', 'adaptive'), 'eta0': (0.1, 0.05, 0.2, 0.15), 'power_t': (0.05, 0.1, 0.2)}
    gs_clf = GridSearchCV(clf, parameters, n_jobs = -1)
    cos_sim = cosSimilarity.reshape(-1,1)
    gs_clf.fit(np.concatenate([cos_sim, b_lda, h_lda], axis=1), labels)
    print (gs_clf.best_score_)
    print (gs_clf.best_params_)
def gs_svmClass2(b_tfidf, h_tfidf, b_lda, h_lda, labels):
    parameters = {'loss': ('hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'), 'penalty': ('l2', 'l1', 'elasticnet')},# 'alpha' : (1e-2, 1e-3, 5e-2, 5e-3), 'learning_rate' : ('constant', 'optimal', 'invscaling', 'adaptive'), 'eta0': (0.1, 0.05, 0.2, 0.15), 'power_t': (0.05, 0.1, 0.2)}
    gs_clf2 = GridSearchCV(clf2, parameters, n_jobs = -1)
    cos_sim = cosSimilarity.reshape(-1,1)
    gs_clf2.fit(np.concatenate([cos_sim, b_lda, h_lda], axis=1), labels)
    print (gs_clf2.best_score_)
    print (gs_clf2.best_params_)

#make predictions
#from sklearn.model_selection import GridSearchCV
#gs_clf = GridSearchCV(n_jobs = -1)
def predict(b_tfidf, h_tfidf, b_lda, h_lda, b_tf, h_tf):
    global predicted_svm
    cos_sim = cosDistance(b_tfidf, h_tfidf).reshape(-1,1)
    cos_sim2 = cosDistance(b_lda, h_lda).reshape(-1,1)
    b_tf2, h_tf2 = b_tf.toarray(), h_tf.toarray()
    predicted_svm = clf.predict(np.concatenate([cos_sim, cos_sim2, b_tf2, h_tf2], axis=1))
    return predicted_svm
def predict2(b_tfidf, h_tfidf, b_lda, h_lda, b_tf, h_tf):
    global predicted_svm
    cos_sim = cosDistance(b_tfidf, h_tfidf).reshape(-1,1)
    cos_sim2 = cosDistance(b_lda, h_lda).reshape(-1,1)    
    b_tf2, h_tf2 = b_tf.toarray(), h_tf.toarray()
    predicted_svm = clf2.predict(np.concatenate([cos_sim, cos_sim2, b_tf2, h_tf2], axis=1))
    return predicted_svm

#Classifier for unrelated and related
# unrelatedRelated(train_data)
# count_vectorizer(train_data['articleBody'], train_data['Headline'])
# b_tf, h_tf = vect.transform(train_data['articleBody']) , vect.transform(train_data['Headline'])
# tfidf_extractor(train_data['articleBody'], train_data['Headline'])
# LDA(b_tf, h_tf)

# svmClass(bodyTFIDF, headlineTFIDF, bodyLDA, headlineLDA, b_tf, h_tf, train_data['new_stance'])
# # train_data = pd.merge(bodies, headlines, on='Body ID')
# train_data['predicted_stance'] = pd.DataFrame(predict(bodyTFIDF, headlineTFIDF, bodyLDA, headlineLDA, b_tf, h_tf))

# #Classifier for agree, diasgree, and discuss
# train_data2 = train_data.loc[train_data['Stance'] != 'unrelated']
# train_data2 = train_data2.reset_index()
# train_data2 = train_data2.drop(columns=['new_stance', 'index', 'predicted_stance'])
# count_vectorizer2(train_data2['articleBody'], train_data2['Headline'])
# b_tf, h_tf = vect2.transform(train_data2['articleBody']) , vect2.transform(train_data2['Headline'])
# tfidf_extractor(train_data2['articleBody'], train_data2['Headline'])
# LDA(b_tf, h_tf)

# svmClass2(bodyTFIDF, headlineTFIDF, bodyLDA, headlineLDA, b_tf, h_tf, train_data2['Stance'])
# train_data2['predicted_stance'] = pd.DataFrame(predict2(bodyTFIDF, headlineTFIDF, bodyLDA, headlineLDA, b_tf, h_tf))

#print the accuracy per category (unrelated, discuss, agree, disagree)
stance_counts = {"unrelated":0, "related":0, "discuss":0, "agree":0, "disagree":0, "FNC":0}
stance_correct_counts = {"unrelated":0, "related":0, "discuss":0, "agree":0, "disagree":0, "FNC":0}

from bs4 import BeautifulSoup as soup
import requests, re
import csv
d = soup(requests.get('https://www.cnn.com/2019/11/02/world/greta-thunberg-leonardo-dicaprio-trnd/index.html').text, 'html.parser')

# articles = list(filter(None, [i.text for i in d.find_all('title', {'class':re.compile('^\w+ _\w+|^\w+$')})]))[2:]
# with open('articles.csv', 'a') as f:
#   write = csv.writer(f)
#   write.writerows([[i] for i in articles])
# with open('articles.csv', newline='') as csvfile:
#   spamreader = csv.reader(csvfile, delimiter=' ', quotechar='|')
#   for row in spamreader:
#     print(', '.join(row))


from bs4 import BeautifulSoup
import urllib.request

def print_list(l):
    for o in l:
        print(o)
    print()

url = "https://www.foxnews.com/us/maryland-driver-gets-probation-for-delaware-crash-that-killed-5-nj-family-members" #THIS IS WHAT YOU CHANGE

page = urllib.request.urlopen(url)
    
soup = BeautifulSoup(page, 'html.parser')

headlines = soup.find_all('h1')
articles = soup.find_all('p')
print_list(headlines)
print_list(articles)

trump_headlines = []
article_list = []
completearticle = ""
headlinetitle = ""
for headline in headlines:
  headlinetitle+=headline.getText()
print(headlinetitle)

for article in articles:
  article_list.append(article.getText())
print_list(article_list)

for article in article_list:
  completearticle += str(article + " ")

print(completearticle)


url =  "https://www.nytimes.com/2019/11/02/sports/trump-ufc-244-nyc.html"
page  =  urllib.request.urlopen(url)
soup = BeautifulSoup(page, 'html.parser')
headlines3 = soup.find_all('h1')
articles3 = soup.find_all('p')
trump_headlines3 = []
article_list3 = []
completearticle3 = ""
headlinetitle3 = ""
for headline3 in headlines3:
  headlinetitle3+=headline3.getText()
print(headlinetitle)

for article3 in articles3:
  article_list3.append(article3.getText())
print_list(article_list3)

for article3 in article_list3:
  completearticle3 += str(article3 + " ")

print(completearticle3)

from bs4 import BeautifulSoup as soup
import requests, re
import csv
d = soup(requests.get('https://www.cnn.com/2019/11/02/world/greta-thunberg-leonardo-dicaprio-trnd/index.html').text, 'html.parser')

# articles = list(filter(None, [i.text for i in d.find_all('title', {'class':re.compile('^\w+ _\w+|^\w+$')})]))[2:]
# with open('articles.csv', 'a') as f:
#   write = csv.writer(f)
#   write.writerows([[i] for i in articles])
# with open('articles.csv', newline='') as csvfile:
#   spamreader = csv.reader(csvfile, delimiter=' ', quotechar='|')
#   for row in spamreader:
#     print(', '.join(row))


from bs4 import BeautifulSoup
import urllib.request

url2 = "https://www.foxnews.com/us/texas-deputies-missing-new-hampshire-couple-murdered-rv-stolen"


page2 = urllib.request.urlopen(url2)
    
soup2 = BeautifulSoup(page2, 'html.parser')

headlines2 = soup2.find_all('h1')
articles2 = soup2.find_all('p')
print_list(headlines2)
print_list(articles2)

trump_headlines2 = []
article_list2 = []
completearticle2 = ""
headlinetitle2 = ""
for headline2 in headlines2:
  headlinetitle2+=headline2.getText()
print(headlinetitle2)

for article2 in articles2:
  article_list2.append(article2.getText())
print_list(article_list2)

for article2 in article_list2:
  completearticle2 += str(article2 + " ")

print(completearticle2)

gaygaygaydatab =  [completearticle3]
gaygaygaydatah = [headlinetitle3]
datadatadatab =  {'Body ID':[696967], 'articleBody':[completearticle3]} 
datah3 = {'Headline':[headlinetitle3], 'Body ID':[696967], 'Stance':['unrelated']}




gaygaydatab = [completearticle2]
gaygaydatah = [headlinetitle2]
datadatab = {'Body ID':[696968], 'articleBody':[completearticle2]} 

frameB2 = pd.DataFrame(datadatab)

datah2 = {'Headline':[headlinetitle2], 'Body ID':[696968], 'Stance':['disagree']}

frameH2 = pd.DataFrame(datah2)


gaydatab =  [completearticle]
gaydatah = [headlinetitle]
datab = {'Body ID':[696969], 'articleBody':[completearticle]} 
datah = {'Headline':[headlinetitle], 'Body ID':[696969], 'Stance':['agree']}
#frameB = pd.DataFrame(datab)
frameB2 =  frameB2.append(pd.DataFrame(datab))
frameH2 = frameH2.append(pd.DataFrame(datah))
frameB2 =  frameB2.append(pd.DataFrame(datadatadatab))
frameH2 = frameH2.append(pd.DataFrame(datah3))
train_dataQ = pd.merge(frameB2, frameH2, on='Body ID')
train_dataQ.append(train_data)

# np.unique()

#frameH = pd.DataFrame(datah)

#Classifier for unrelated and related

unrelatedRelated(train_dataQ)
count_vectorizer(train_dataQ['articleBody'], train_dataQ['Headline'])
b_tf, h_tf = vect.transform(train_dataQ['articleBody']) , vect.transform(train_dataQ['Headline'])
tfidf_extractor(train_dataQ['articleBody'], train_dataQ['Headline'])
LDA(b_tf, h_tf)

svmClass(bodyTFIDF, headlineTFIDF, bodyLDA, headlineLDA, b_tf, h_tf, train_dataQ['new_stance'])
train_dataQ['predicted_stance'] = pd.DataFrame(predict(bodyTFIDF, headlineTFIDF, bodyLDA, headlineLDA, b_tf, h_tf))

print(train_dataQ['predicted_stance'])

# Create DataFrame 
# gayb = pd.DataFrame(gaydatab)
# gayh = pd.DataFrame(gaydatah) 

# correct_percentages = test_predictionsUnrelated(train_data)
# print(correct_percentages)
# correct_percentages = test_predictionsRelated(train_data2)
# print(correct_percentages)